{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ESCUELA POLITÉCNICA NACIONAL\n",
    "# FACULTAD DE INGENIERÍA EN SISTEMAS\n",
    "\n",
    "## Proyecto Bimestral: Sistema de Recuperación de Información basado en Reuters-21578\n",
    "\n",
    "### Autor: Soto Condoy Bruce Andrés"
   ],
   "id": "253a70f89530852c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. INTRODUCIÓN\n",
    "El objetivo de este proyecto es diseñar, construir, programar y desplegar un Sistema de Recuperación de Información (SRI) utilizando el corpus Reuters-21578. El proyecto se dividirá en varias fases, que se describen a continuación.\n",
    "\n"
   ],
   "id": "e4d459e566a6c0ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. FASES DEL PROYECTO\n",
    "Para realizar el proyecto se ocuparon libreías, las cuales seran de mucha ayuda al momento de realizar cada una de las fases del proyecto. Antes de continuar se muestran cada una de ellas y su respectiva función"
   ],
   "id": "7e364e663dbae0ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Importación de librerías\n",
   "id": "f0fd639b5ba94c09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T02:59:30.401972Z",
     "start_time": "2024-06-20T02:59:29.055225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os  #modulo para interactuar con el sistema operativo\n",
    "import numpy as np  #biblioteca para arrays y funciones matematicas\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  #funciones para conteo de palabras y usar TF-IDF\n",
    "import pandas as pd  #biblioteca para analisis y manipulacion de datos\n",
    "from collections import defaultdict  #clase para diccionarios\n",
    "import json  #modulo para trabajar con datos en formato JSON\n",
    "import re  #modulo para manipulacion de texto\n",
    "import nltk  #biblioteca para procesamiento del lenguaje natural\n",
    "from nltk.tokenize import word_tokenize  # funcioun para dividir texto en palabras\n",
    "from nltk.stem import PorterStemmer  #clase para reducir palabras a su raiz\n",
    "from sklearn.metrics.pairwise import cosine_similarity  #funcion para calcular similitud coseno\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score  #funciones para evaluar modelos de clasificacion"
   ],
   "id": "f4223d33294a93ba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  2.1. Adquisición de Datos\n",
    "Se descargó el archivo reuter.rar contenido en el url: https://github.com/ivan-carrera/ir24a/blob/main/proj01/data/reuters.rar <br>\n",
    "Posterior a eso se descomprimieron todos los archivos dentro del directorio para seguir con el proyecto."
   ],
   "id": "bf9b0192271847b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###  2.2. Preprocesamiento",
   "id": "19421cdca42b957"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Obtención de las stopwords\n",
    "Para obetener las stopwords se uso el archivo *stopwords* que se encuentra en el directorio y se las guardo en el array \"stop_words\""
   ],
   "id": "fa883c757bab6ce4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T02:59:30.416384Z",
     "start_time": "2024-06-20T02:59:30.403244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('data/reuters/stopwords', 'r') as file:\n",
    "    content = file.read()\n",
    "    stop_words = content.split('\\n')\n",
    "\n",
    "stop_words = [word.strip() for word in stop_words if word.strip()]\n",
    "print(stop_words[:])"
   ],
   "id": "7799b7ae99bf3966",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\", \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', \"hadn't\", 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he's\", 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', \"t's\", 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there's\", 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\", 'whatever', 'when', 'whence', 'whenever', 'where', \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', \"who's\", 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'would', 'would', \"wouldn't\", 'x', 'y', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Preprocesamiento de documentos\n",
    "Dentro de la funcion \"preprocess_document\" se realiza lo siguiente:\n",
    " -  Se eliminan caracteres no deseados y se normaliza el texto (se convierte todo a minúsculas).\n",
    " - Se divide el texto en palabras utilizando word_tokenize de NLTK.\n",
    " - Se eliminan las stop words y se aplica stemming utilizando PorterStemmer de NLTK."
   ],
   "id": "be661bc18e9113e9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-20T02:59:30.433250Z",
     "start_time": "2024-06-20T02:59:30.418391Z"
    }
   },
   "source": [
    "ps = PorterStemmer() #definir el stemmer fuera ya que se usara en el resto del codigo\n",
    "def preprocess_document(content):\n",
    "    text = re.sub(r'\\s+', ' ', content)  #remover espacios extra\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  #mantener solo caracteres alfabeticos\n",
    "    text = text.lower()  #convertir a minusculas\n",
    "    \n",
    "    tokens = word_tokenize(text, language='english') #tokenizacion\n",
    "    \n",
    "    tokens = [ps.stem(word) for word in tokens if word not in stop_words and len(word) > 1] #eliminar stop words y aplicar stemming\n",
    "\n",
    "    return tokens"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Preprocesamiento de archivos\n",
    "La función \"preprocess_files\" recorre todos los archivos en el directorio de entrada, los procesa utilizando \"preprocess_document\", y guarda los resultados preprocesados en la carpeta de salida *data/reuters/preprocessed_data*."
   ],
   "id": "499da1c5c3c2385f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T02:59:50.236900Z",
     "start_time": "2024-06-20T02:59:30.436256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_files(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if '.' not in filename:\n",
    "            with open(os.path.join(input_folder, filename), 'r', encoding='latin1') as file:\n",
    "                content = file.read()\n",
    "                tokens = preprocess_document(content)\n",
    "                output_filename = os.path.join(output_folder, f\"{filename}\")\n",
    "                with open(output_filename, 'w') as output_file:\n",
    "                    output_file.write(' '.join(tokens))\n",
    "                    \n",
    "preprocess_files('data/reuters/training', 'data/reuters/preprocessed_data') #llamada a la funcion indicando el path del archivo donde se encuentran los datos y el path en donde queremos que se guarden los archivos procesados"
   ],
   "id": "90877e97e5a8284e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Lectura de datos preprocesados\n",
    "La función \"read_preprocessed_files\" lee los documentos preprocesados desde la carpeta especificada y devuelve una lista de documentos y sus nombres de archivo."
   ],
   "id": "ad0f83b6986f7c3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T02:59:56.853944Z",
     "start_time": "2024-06-20T02:59:50.236900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_preprocessed_files(folder):\n",
    "    documents = []\n",
    "    filenames = []\n",
    "    \n",
    "    for filename in os.listdir(folder):\n",
    "        if '.' not in filename:\n",
    "            with open(os.path.join(folder, filename), 'r', encoding='latin1') as file:\n",
    "                content = file.read()\n",
    "                documents.append(content)\n",
    "                filenames.append(filename)\n",
    "    return documents, filenames\n",
    "\n",
    "documents, filenames = read_preprocessed_files('data/reuters/preprocessed_data')\n"
   ],
   "id": "83b8264ff40a8761",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3. Representación de Datos en Espacio Vectorial",
   "id": "a8a6173bf20b865d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Aplicar Bag of Words (BoW)\n",
    "La función \"apply_bow\" utiliza CountVectorizer de scikit-learn para vectorizar los documentos usando la técnica BoW.<br>\n",
    "<br>\n",
    "#### Aplicar TF-IDF\n",
    "La función \"apply_tfidf\" utiliza TfidfVectorizer de scikit-learn para vectorizar los documentos usando la técnica TF-IDF.<br>"
   ],
   "id": "1ef0df5cc151dec3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T02:59:59.912567Z",
     "start_time": "2024-06-20T02:59:56.857871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#aplicar BoW\n",
    "def apply_bow(documents):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    return X, vectorizer\n",
    "\n",
    "#aplicar TF-IDF\n",
    "def apply_tfidf(documents):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    return X, vectorizer\n",
    "\n",
    "X_bow, bow_vectorizer = apply_bow(documents)\n",
    "X_tfidf, tfidf_vectorizer = apply_tfidf(documents)\n"
   ],
   "id": "93cdabc04778f65e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f5ab9f7d48c53670"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.4. Indexación",
   "id": "c095dd7ada7bc4a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Construcción del índice invertido\n",
    "En la función \"build_inverted_index\" se usa *defaultdict* de la biblioteca collections para almacenar el índice invertido. Para cada término en cada documento, si el valor del término es mayor que 0, agregamos el identificador del documento a la lista correspondiente en el diccionario."
   ],
   "id": "c6d23ff5f795c403"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T02:59:59.918144Z",
     "start_time": "2024-06-20T02:59:59.913766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_inverted_index(X, vectorizer, filenames):\n",
    "    inverted_index = defaultdict(list)\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    for doc_id, doc in enumerate(X.toarray()):\n",
    "        for term_id, term_freq in enumerate(doc):\n",
    "            if term_freq > 0:\n",
    "                term = terms[term_id]\n",
    "                inverted_index[term].append(filenames[doc_id])\n",
    "    return inverted_index\n"
   ],
   "id": "eab1b69f4174dcf7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Guardado del índice invertido\n",
    "El índice invertido se guarda en la función \"save_inverted_index\" en un archivo JSON para facilitar su uso en futuras búsquedas."
   ],
   "id": "9429ad7973bcd525"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T02:59:59.924249Z",
     "start_time": "2024-06-20T02:59:59.919302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_inverted_index(inverted_index, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(inverted_index, f, indent=4) #guardar indice invertido en un archivo JSON"
   ],
   "id": "ae391b3e867c19c8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Llamado de las funciones para construir y guardar el índice invertido",
   "id": "31b686b1feec16ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:00:54.225307Z",
     "start_time": "2024-06-20T02:59:59.925501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#construir y guardar indice invertido para BoW\n",
    "inverted_index_bow = build_inverted_index(X_bow, bow_vectorizer, filenames)\n",
    "save_inverted_index(inverted_index_bow, 'data/reuters/inverted_index_bow.json')\n",
    "\n",
    "# construir y guardar indice invertido para TF-IDF\n",
    "inverted_index_tfidf = build_inverted_index(X_tfidf, tfidf_vectorizer, filenames)\n",
    "save_inverted_index(inverted_index_tfidf, 'data/reuters/inverted_index_tfidf.json')"
   ],
   "id": "19a675208fbec279",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.5. Diseño del Motor de Búsqueda",
   "id": "a3b8c217136d31c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Preprocesamiento de la Consulta\n",
    "La función \"preprocess_query\" limpia el texto de la consulta eliminando caracteres no alfabéticos y espacios extra, convirtiendo todo a minúsculas, y aplicando tokenización, eliminación de stop words y stemming. Esto ayuda a normalizar la consulta para mejorar la precisión en la búsqueda."
   ],
   "id": "f8d61e9342337999"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:00:54.234753Z",
     "start_time": "2024-06-20T03:00:54.227319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_query(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  #remover espacios extra\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  #mantener solo caracteres alfabeticos\n",
    "    text = text.lower()  #convertir a minusculas\n",
    "\n",
    "    tokens_query = word_tokenize(text, language='english') #tokenizacion\n",
    "\n",
    "    tokens_query = [ps.stem(word) for word in tokens_query if word not in stop_words and len(word) > 1] #eliminar stop words y aplicar stemming\n",
    "    \n",
    "    return ' '.join(tokens_query)"
   ],
   "id": "b21ae33b8ecde9f8",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Búsqueda de Documentos Relevantes\n",
    "La función \"search\" toma una consulta y busca documentos relevantes utilizando un índice invertido y un vectorizador. Preprocesa la consulta, encuentra documentos relevantes, calcula similitudes de coseno, y devuelve los documentos más relevantes ordenados por similitud, limitándose a aquellos con una similitud mayor a 0.35."
   ],
   "id": "1758e616e7046fad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T04:05:02.451436Z",
     "start_time": "2024-06-20T04:05:02.426447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def search(query, inverted_index, vectorizer, X, filenames, top_n=None):\n",
    "    query = preprocess_query(query)\n",
    "    query_terms = query.split()\n",
    "    \n",
    "    filename_to_index = {filename: idx for idx, filename in enumerate(filenames)} #mapear nombres de archivos a sus indices\n",
    "    relevant_docs = set()\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            relevant_docs.update(inverted_index[term])\n",
    "    relevant_docs = [filename_to_index[doc] for doc in relevant_docs if doc in filename_to_index] #convertir nombres de archivos relevantes a indices\n",
    "    if not relevant_docs:\n",
    "        return [], []\n",
    "    X_relevant = X[relevant_docs]\n",
    "    filenames_relevant = [filenames[i] for i in relevant_docs]\n",
    "\n",
    "    #procesar la consulta y calcular similitudes\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vector, X_relevant).flatten()\n",
    "    \n",
    "    #ordenar documentos por similitud y limitar a los que tengan una siminitud de mas del 35%\n",
    "    ranked_indices = np.argsort(similarities)[::-1][:]\n",
    "    ranked_filenames = [filenames_relevant[i] for i in ranked_indices if similarities[i] > 0.35]\n",
    "    \n",
    "    return ranked_filenames, similarities[ranked_indices]\n",
    "\n",
    "#ejemplo\n",
    "query = \"soybean oilseed\"\n",
    "\n",
    "ranked_filenames_bow, similarities_bow = search(query,inverted_index_bow, bow_vectorizer, X_bow, filenames)\n",
    "ranked_filenames_tfidf, similarities_tfidf = search(query, inverted_index_tfidf, tfidf_vectorizer, X_tfidf, filenames)\n",
    "\n",
    "#mostrar resultados\n",
    "print(\"Resultados para BoW:\")\n",
    "for filename, similarity in zip(ranked_filenames_bow, similarities_bow):\n",
    "    category = ''\n",
    "    if filename in processed_categories:\n",
    "        category = processed_categories[filename]\n",
    "    print(f\"{filename} {similarity}{category}\")\n",
    "print(\"\\nResultados para TF-IDF:\")\n",
    "for filename, similarity in zip(ranked_filenames_tfidf, similarities_tfidf):\n",
    "    category = ''\n",
    "    if filename in processed_categories:\n",
    "        category = processed_categories[filename]\n",
    "    print(f\"{filename} {similarity}{category}\")"
   ],
   "id": "7416ec78835d9b6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para BoW:\n",
      "5702 0.46694217336895677['oilse', 'soybean']\n",
      "9617 0.44189395817353083['soybean', 'oilse']\n",
      "7356 0.3926035266493783['soybean', 'oilse']\n",
      "6906 0.3598560863424403['soybean', 'oilse']\n",
      "\n",
      "Resultados para TF-IDF:\n",
      "3540 0.46133600066185043['coconut oil', 'palm oil', 'veg oil', 'soybean', 'oilse']\n",
      "5702 0.4584567560163928['oilse', 'soybean']\n",
      "9617 0.42097743338966576['soybean', 'oilse']\n",
      "7356 0.36308960159507786['soybean', 'oilse']\n",
      "3888 0.36126966080973694['oilse']\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.6. Evaluación del Sistema\n",
    "Para la evalucion del sistema se debe usar el archivo *cats.txt* el cual contiene cada una de las categorías a las que pertenece cada archivo, con esto se puede usar las metricas de evaluación como: recall, precisión y F1-score"
   ],
   "id": "f7aa434233da8f52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Categorías\n",
    "La función \"load_categories\" carga las categorías de los documentos en un diccionario, mapeando cada nombre de archivo a una lista de categorías."
   ],
   "id": "9336a7d93f86d880"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:00:54.425336Z",
     "start_time": "2024-06-20T03:00:54.288592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_categories(file):\n",
    "    categories = {}\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 1:\n",
    "                categories[parts[0]] = parts[1:]\n",
    "    return categories\n",
    "categories = load_categories('data/reuters/cats.txt')\n",
    "categories = {key: value for key, value in categories.items() if 'training' in key} #obtener solo de los documentos de training"
   ],
   "id": "9119e366eaf8c177",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Preprocesado de categorías\n",
    "La función \"preprocess_categories\" limpia las categorías de los documentos eliminando caracteres no alfabéticos y espacios extra, convirtiendo todo a minúsculas, y aplicando tokenización, eliminación de stop words y stemming."
   ],
   "id": "ae5b528ebf33995c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T03:00:55.059547Z",
     "start_time": "2024-06-20T03:00:54.427356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_categories(categories):\n",
    "    processed_categories = {}\n",
    "    for filename, cats in categories.items():\n",
    "        processed_cats = []\n",
    "        for cat in cats:\n",
    "            cat = re.sub(r'\\s+', ' ', cat) #remover espacios extra\n",
    "            cat = re.sub(r'[^a-zA-Z]', ' ', cat) #mantener solo caracteres alfabeticos\n",
    "            cat = cat.lower() #convertir a minusculas\n",
    "            \n",
    "            tokens = word_tokenize(cat, language='english') #tokenizacion\n",
    "            \n",
    "            tokens = [ps.stem(word) for word in tokens if word not in stop_words and len(word) > 1] #eliminar stop words y aplicar stemming\n",
    "            \n",
    "            processed_cat = ' '.join(tokens)\n",
    "            \n",
    "            if processed_cat:\n",
    "                processed_cats.append(processed_cat)\n",
    "        \n",
    "        base_filename = filename.split('/')[-1] #pasar de 'training/1' a '1'\n",
    "        processed_categories[base_filename] = processed_cats\n",
    "        \n",
    "    return processed_categories\n",
    "\n",
    "processed_categories = preprocess_categories(categories)"
   ],
   "id": "c28420e59ac9be58",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Evaluación del sistema\n",
    "La función \"evaluate_search_system\" toma un conjunto de consultas y un vectorizador y calcula las métricas de precisión, recall y F1-score para el sistema de búsqueda. Para cada consulta, procesa la consulta, busca documentos relevantes, obtiene las categorías reales de los documentos relevantes, y calcula las métricas de precisión, recall y F1-score."
   ],
   "id": "ba8407e85ec03500"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T04:21:55.856804Z",
     "start_time": "2024-06-20T04:21:55.008309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_search_system(query_set, vectorizer, X, filenames, inverted_index):\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for query in query_set:\n",
    "        query_processed = preprocess_query(query)\n",
    "        \n",
    "        ranked_filenames, _ = search(query, inverted_index, vectorizer, X, filenames) #obtener documentos relevantes\n",
    "        \n",
    "        #obtener las categorias reales de los documentos relevantes\n",
    "        actual_categories = []\n",
    "        for filename in ranked_filenames:\n",
    "            if filename in processed_categories:\n",
    "                actual_categories.extend(processed_categories[filename])\n",
    "\n",
    "        predicted_categories = [query_processed] * len(actual_categories) #todas las categorias predichas son las mismas de la query, ya que al devolverme los documentos relevantes lo toma de esa forma\n",
    "        \n",
    "        #calculo de metricas\n",
    "        if len(actual_categories) == 0 and len(predicted_categories) == 0:\n",
    "            precision = 1.0\n",
    "            recall = 1.0\n",
    "            f1 = 1.0\n",
    "        else:\n",
    "            precision = precision_score(actual_categories, predicted_categories, average='micro', zero_division=1)\n",
    "            recall = recall_score(actual_categories, predicted_categories, average='micro', zero_division=1)\n",
    "            f1 = f1_score(actual_categories, predicted_categories, average='micro', zero_division=1)\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    #promedios de las metricas\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "    return avg_precision, avg_recall, avg_f1\n",
    "\n",
    "#evaluar con todas las categorias\n",
    "all_categories = set()\n",
    "for cats in categories.values():\n",
    "    all_categories.update(cats)\n",
    "query_set = list(all_categories)\n",
    "\n",
    "precision_bow, recall_bow, f1_bow = evaluate_search_system(query_set, bow_vectorizer, X_bow, filenames, inverted_index_bow)\n",
    "precision_tfidf, recall_tfidf, f1_tfidf = evaluate_search_system(query_set, tfidf_vectorizer, X_tfidf, filenames, inverted_index_tfidf)\n",
    "\n",
    "print(\"Resultados de evaluacion usando BoW:\")\n",
    "print(f\"Precision: {precision_bow:.4f}\")\n",
    "print(f\"Recall: {recall_bow:.4f}\")\n",
    "print(f\"F1-score: {f1_bow:.4f}\")\n",
    "\n",
    "print(\"\\nResultados de evaluacion usando TF-IDF:\")\n",
    "print(f\"Precision: {precision_tfidf:.4f}\")\n",
    "print(f\"Recall: {recall_tfidf:.4f}\")\n",
    "print(f\"F1-score: {f1_tfidf:.4f}\")"
   ],
   "id": "4b8263ca728f2824",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de evaluacion usando BoW:\n",
      "Precision: 0.6482\n",
      "Recall: 0.6482\n",
      "F1-score: 0.6482\n",
      "\n",
      "Resultados de evaluacion usando TF-IDF:\n",
      "Precision: 0.6538\n",
      "Recall: 0.6538\n",
      "F1-score: 0.6538\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Servidor con flask",
   "id": "a3fa3ece0fde116a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__) #guardar servidor en app\n",
    "CORS(app)  #permitir solicitudes CORS de cualquier origen\n",
    "\n",
    "api: str = '/api/v1/' #nombre de la api\n",
    "\n",
    "#flask routing\n",
    "@app.route(api+'', methods=['GET'])\n",
    "def hello():\n",
    "    response = {'hello': \"Hello world..\"}\n",
    "    return jsonify(response)\n",
    "\n",
    "@app.route(api+'search', methods=['POST'])\n",
    "def buscar():\n",
    "    data = request.get_json()\n",
    "    query = data.get('query')\n",
    "    results = {'results': f'Resultados para la búsqueda: {query}'}\n",
    "    return jsonify(results)\n",
    "\n",
    "#servidor principal\n",
    "if __name__ == '__main__':\n",
    "    app.run() #si se realiza camabios se actualiza automaticamente"
   ],
   "id": "d334ac40493aea31",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
